

# ---------- Load cache (if exists and fresh) ----------
def load_cache():
    if not os.path.exists(CACHE_FILE):
        return {"timestamp": 0, "results": []}
    try:
        with open(CACHE_FILE, "r") as f:
            data = json.load(f)
        if time.time() - data.get("timestamp", 0) < CACHE_TTL:
            print("âœ… Loaded cache from file")
            return data
        else:
            print("â™»ï¸ Cache file expired â€” refreshing")
            return {"timestamp": 0, "results": []}
    except Exception:
        return {"timestamp": 0, "results": []}


def save_cache(data):
    try:
        with open(CACHE_FILE, "w") as f:
            json.dump(data, f)
        print("ðŸ’¾ Cache saved to file")
    except Exception as e:
        print(f"âš ï¸ Failed to save cache: {e}")


cache_data = load_cache()


def get_sp500_tickers():
    url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/130.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "en-US,en;q=0.9",
    }

    response = requests.get(url, headers=headers, timeout=10)
    response.raise_for_status()

    # Sanity check to ensure we actually got a Wikipedia page
    if "<table" not in response.text or "Symbol" not in response.text:
        raise RuntimeError("Wikipedia page returned unexpected content")

    tables = pd.read_html(response.text)
    for table in tables:
        for col in table.columns:
            if "symbol" in str(col).lower() or "ticker" in str(col).lower():
                tickers = table[col].astype(str).str.replace(".", "-", regex=False).tolist()
                return tickers
    raise ValueError("No Symbol/Ticker column found in Wikipedia table.")


# ---------- STEP 2: Compute changes ----------
def compute_changes(batch):
    results = []
    try:
        data = yf.download(batch, period="6mo", interval="1d", group_by="ticker", progress=False)
        for ticker in batch:
            try:
                close = data[ticker]["Close"]
                if close.empty:
                    continue

                start_6m, end_6m = close.iloc[0], close.iloc[-1]
                change_6m = (end_6m - start_6m) / start_6m * 100 if start_6m else None

                mid_index = int(len(close) * 0.5)
                start_3m, end_3m = close.iloc[mid_index], close.iloc[-1]
                change_3m = (end_3m - start_3m) / start_3m * 100 if start_3m else None

                results.append({
                    "symbol": ticker,
                    "change_3m_pct": round(change_3m, 2) if change_3m is not None else None,
                    "change_6m_pct": round(change_6m, 2) if change_6m is not None else None
                })
            except Exception:
                continue
    except Exception:
        pass
    return results


# ---------- STEP 3: Batch + parallel processing ----------
def get_top_gainers_data():
    tickers = get_sp500_tickers()
    batches = [tickers[i:i + BATCH_SIZE] for i in range(0, len(tickers), BATCH_SIZE)]
    all_results = []

    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        futures = [executor.submit(compute_changes, batch) for batch in batches]
        for f in as_completed(futures):
            all_results.extend(f.result())

    df = pd.DataFrame(all_results).dropna(subset=["change_6m_pct"])
    df["change_6m_pct"] = df["change_6m_pct"].astype(float)
    df = df.sort_values("change_6m_pct", ascending=False).head(20)
    return df.to_dict(orient="records")


# ---------- STEP 4: Flask Endpoint ----------
@app.route("/api/top_gainers", methods=["GET"])
def top_gainers():
    global cache_data
    now = time.time()

    # Serve cached data if fresh
    if cache_data["results"] and (now - cache_data["timestamp"] < CACHE_TTL):
        print("âœ… Serving from cache (memory)")
        return jsonify(cache_data["results"])

    # Refresh and save
    print("â™»ï¸ Refreshing data (cache expired or empty)...")
    data = get_top_gainers_data()
    cache_data = {"timestamp": now, "results": data}
    save_cache(cache_data)
    return jsonify(data)
